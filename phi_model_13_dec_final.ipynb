{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaabbas/Humor-generation-colab-notebook/blob/main/phi_model_13_dec_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7c1w_ubqthg"
      },
      "outputs": [],
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install -q -U transformers peft accelerate bitsandbytes datasets\n",
        "\n",
        "# 2. Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the absolute path for persistent storage\n",
        "# CRITICAL: This path MUST exist in your Google Drive!\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Mistral_Jokes/Training_Results\"\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True) # Ensure the directory exists (for safety)\n",
        "\n",
        "print(f\"Checkpoints and final adapter will be saved to: {DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, Dataset # <-- Ensure Dataset is imported\n",
        "import os\n",
        "import pandas as pd # <-- Used to handle the TSV file structure\n",
        "\n",
        "# 1. Model Name (Phi-2 for 3-hour training)\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "# --- ðŸŽ¯ CRITICAL STEP: Load Your Custom TSV Dataset ---\n",
        "\n",
        "# ðŸ“Œ File Path: Loaded from the temporary Colab session storage as you requested.\n",
        "DATASET_FILE_PATH = \"/content/final_dataset_fixed\"\n",
        "\n",
        "try:\n",
        "    print(f\"Loading TSV file from local session path: {DATASET_FILE_PATH}\")\n",
        "\n",
        "    # Use pandas to read the TSV file.\n",
        "    # Assumes tab separation (\\t), no initial header, and we define the names.\n",
        "    df = pd.read_csv(\n",
        "        DATASET_FILE_PATH,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['id', 'input', 'joke'],\n",
        "        skiprows=1 # Skip the header line based on your provided example\n",
        "    )\n",
        "\n",
        "    # Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # ðŸŽ¯ Data Formatting Function (Crucial for Fine-Tuning)\n",
        "    # This combines the 'input' and 'joke' columns into the required C-LM format.\n",
        "    def format_joke_example(example):\n",
        "        # Format: \"### Input: [input_text]\\n### Joke: [joke_text]\"\n",
        "        # This is the single 'text' column that will be tokenized in Cell 3.\n",
        "        example['text'] = f\"### Input: {example['input']}\\n### Joke: {example['joke']}\"\n",
        "        return example\n",
        "\n",
        "    # Apply the formatting and remove the old columns\n",
        "    # ðŸ’¥ FIX APPLIED: Removed the non-existent '__index_level_0__'\n",
        "    dataset = dataset.map(\n",
        "        format_joke_example,\n",
        "        remove_columns=['id', 'input', 'joke']\n",
        "    )\n",
        "\n",
        "    # Split the dataset into training and evaluation sets (90/10 split)\n",
        "    dataset_split = dataset.train_test_split(test_size=0.10, seed=42)\n",
        "\n",
        "    train_dataset = dataset_split['train']\n",
        "    eval_dataset = dataset_split['test']\n",
        "\n",
        "    print(f\"Successfully loaded {len(train_dataset)} examples for training and {len(eval_dataset)} for evaluation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load or process the dataset from {DATASET_FILE_PATH}\")\n",
        "    print(\"Please ensure your file is uploaded and named 'final_dataset_fixed' in the /content/ folder.\")\n",
        "    raise e\n",
        "\n",
        "# --- End of Dataset Loading ---\n",
        "\n",
        "# 2. Quantization Configuration (QLoRA)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 3. Load Base Model and Tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 4. Prepare Model for QLoRA Training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 5. LoRA Configuration (PEFT)\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=32,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "    ],\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "LuWZsOWhynbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# NOTE: DRIVE_PATH must have been defined in Cell 1\n",
        "TENSORBOARD_LOG_DIR = os.path.join(DRIVE_PATH, \"runs\")\n",
        "\n",
        "print(f\"TensorBoard will monitor logs in: {TENSORBOARD_LOG_DIR}\")\n",
        "print(\"If the panel shows 'No dashboards are active', it will update once training begins and the first logging step (25) is reached.\")\n",
        "\n",
        "# Use Javascript to make the Colab notebook execute the magic command\n",
        "# This launches the TensorBoard visualization panel below this cell.\n",
        "js_code = f\"\"\"\n",
        "  IPython.notebook.execute_cells_below();\n",
        "  google.colab.kernel.invokeFunction('notebook.runCell', [\n",
        "    '%%tensorboard --logdir={TENSORBOARD_LOG_DIR}'\n",
        "  ], {{}});\n",
        "\"\"\"\n",
        "\n",
        "display(Javascript(js_code))"
      ],
      "metadata": {
        "id": "95qQ-wPcyqox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import accelerate\n",
        "import torch\n",
        "import os\n",
        "import glob # Needed for dynamic checkpoint loading\n",
        "\n",
        "# --- Tokenize the Datasets ---\n",
        "# The tokenize_function uses the 'text' column created in Cell 2\n",
        "def tokenize_function(examples):\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512\n",
        "    )\n",
        "    # Causal Language Modeling: The labels are the input IDs\n",
        "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
        "    return tokenized_output\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "# Assumes train_dataset and eval_dataset were defined in Cell 2\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# --- Data Collator ---\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# --- Training Arguments (OPTIMIZED for PHI-2 and 3-Hour Finish) ---\n",
        "training_args = TrainingArguments(\n",
        "    # ðŸ’¥ CRITICAL: This is the Google Drive path defined in Cell 1\n",
        "    output_dir=DRIVE_PATH,\n",
        "    num_train_epochs=2, # Reduced epochs for 3-hour speed target\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=25,\n",
        "\n",
        "    # Checkpoint and Evaluation Strategy\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,  # Saves checkpoint roughly every 30-40 minutes\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,  # Evaluates loss every 1000 steps\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    warmup_ratio=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    run_name=\"phi2-joke-generator-qlora\",\n",
        ")\n",
        "\n",
        "# --- Trainer Initialization ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# --- Trainer Start (DYNAMIC RESUME LOGIC) ---\n",
        "# Check for existing checkpoints in the Google Drive path\n",
        "checkpoints = glob.glob(os.path.join(DRIVE_PATH, \"checkpoint-*\"))\n",
        "LATEST_CHECKPOINT_PATH = None\n",
        "\n",
        "if checkpoints:\n",
        "    checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
        "    LATEST_CHECKPOINT_PATH = checkpoints[-1]\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "if LATEST_CHECKPOINT_PATH:\n",
        "    # Resumes if a checkpoint was found\n",
        "    print(f\"Resuming training from {LATEST_CHECKPOINT_PATH}...\")\n",
        "    trainer.train(resume_from_checkpoint=LATEST_CHECKPOINT_PATH)\n",
        "\n",
        "else:\n",
        "    # Starts from step 0 if no checkpoints exist\n",
        "    print(\"No checkpoints found. Starting training from step 0...\")\n",
        "    trainer.train()\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# --- Save the Final Fine-Tuned Adapter to Google Drive ---\n",
        "FINAL_ADAPTER_PATH = os.path.join(DRIVE_PATH, \"final_humor_generator_adapter\")\n",
        "trainer.model.save_pretrained(FINAL_ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(FINAL_ADAPTER_PATH)\n",
        "print(f\"Final adapter saved to: {FINAL_ADAPTER_PATH}\")"
      ],
      "metadata": {
        "id": "_rvrqidKyu6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING MODEL"
      ],
      "metadata": {
        "id": "TxAinS199J0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Assuming 'model' and 'tokenizer' are defined from your setup cells\n",
        "print(\"--- Starting Spot-Check Generation ---\")\n",
        "\n",
        "# Setup the text generation pipeline\n",
        "# Note: Since the model is currently a PEFT model, we must pass it directly.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Your custom prompt template structure (modified for the Phi-2 input)\n",
        "HUMOR_PROMPT_TEMPLATE = \"### Input: {input_text}\\n### Joke: \"\n",
        "\n",
        "def generate_test_joke(input_text, generator, template):\n",
        "    \"\"\"Generates a joke and cleans the output based on the template.\"\"\"\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = template.format(input_text=input_text)\n",
        "\n",
        "    # Generate the text\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=64, # Generate a short joke\n",
        "        do_sample=True,\n",
        "        temperature=0.85, # Use sampling for creative, diverse jokes\n",
        "        top_p=0.9,\n",
        "        return_full_text=False # Return only the generated part\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text'].strip()\n",
        "\n",
        "    # Attempt to clean up the output to get only the joke text\n",
        "    if generated_text.startswith(prompt):\n",
        "        joke = generated_text[len(prompt):].strip()\n",
        "    else:\n",
        "        joke = generated_text\n",
        "\n",
        "    return joke\n",
        "\n",
        "# --- Test Inputs (Use examples similar to your TSV data) ---\n",
        "test_inputs = [\n",
        "    # Headline style input\n",
        "    \"British expats in France hit with shock pension tax bills\",\n",
        "    \"Punches and slaps: Watch as Mexican Senate debate ends in brawl\",\n",
        "    # Word pair style input\n",
        "    \"spray chair\",\n",
        "    \"hammer banana\",\n",
        "    \"microwave book\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Generated Jokes ---\")\n",
        "for input_text in test_inputs:\n",
        "    joke = generate_test_joke(input_text, generator, HUMOR_PROMPT_TEMPLATE)\n",
        "    print(f\"Input: {input_text}\")\n",
        "    print(f\"Joke:  {joke}\\n\")\n",
        "\n",
        "print(\"--- Spot-Check Complete ---\")"
      ],
      "metadata": {
        "id": "m8locfgQwvDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MERGING MODEL"
      ],
      "metadata": {
        "id": "3Ur8jXsM9N1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 1. CONFIGURATION (VERIFY THESE PATHS) ---\n",
        "\n",
        "# The base model ID used for your QLoRA training\n",
        "BASE_MODEL_ID = \"microsoft/phi-2\"\n",
        "\n",
        "# ðŸ’¥ The path where your final adapter weights were saved (VERIFY THIS!)\n",
        "# Output from your successful training run:\n",
        "FINAL_ADAPTER_PATH = \"/content/drive/MyDrive/Mistral_Jokes/Training_Results/final_humor_generator_adapter\"\n",
        "\n",
        "# The local directory where the final, merged model will be saved\n",
        "# This is the directory you will upload to Hugging Face\n",
        "MERGED_MODEL_DIR = \"/content/phi2_humor_merged_model\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(MERGED_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# --- 2. MODEL MERGING PROCESS ---\n",
        "\n",
        "print(\"Starting Model Merging Process...\")\n",
        "\n",
        "# A. Load the base model and tokenizer in the correct format (FP16 or BF16 recommended)\n",
        "# We load it without quantization config this time, as we want the full weights.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16, # Use BF16 if available, or FP16 (phi-2 works well with this)\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
        "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "# B. Load the fine-tuned PEFT (LoRA) adapter\n",
        "# This wraps the adapter around the base model\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    FINAL_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "# C. Merge the adapter weights into the base model weights\n",
        "# This creates a single, self-contained model that does not require the PEFT library for inference.\n",
        "print(\"Merging adapter weights into the base model...\")\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "merged_model.to(torch.bfloat16).eval() # Ensure it's in eval mode and BFloat16\n",
        "\n",
        "# D. Save the final merged model\n",
        "merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
        "base_tokenizer.save_pretrained(MERGED_MODEL_DIR)\n",
        "\n",
        "print(\"\\n--- MERGE SUCCESSFUL! ---\")\n",
        "print(f\"The deployable model is saved to: {MERGED_MODEL_DIR}\")\n",
        "print(\"\\nYour next step is to upload this directory to Hugging Face.\")"
      ],
      "metadata": {
        "id": "M6QvN203xU6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUSHING TO HUGGING FACE"
      ],
      "metadata": {
        "id": "n068Uz9-9SKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "# Paste the NEW 'Write' token into the widget."
      ],
      "metadata": {
        "id": "O1qRUgOr1-9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# --- 1. CONFIGURATION (VERIFY THESE) ---\n",
        "# The local directory containing the merged Phi-2 model\n",
        "MERGED_MODEL_DIR = \"/content/phi2_humor_merged_model\"\n",
        "\n",
        "# Your target repository ID (User/RepoName)\n",
        "REPO_ID = \"insaabbas/phi2_humor_merged_model\"\n",
        "\n",
        "# --- 2. UPLOAD CODE ---\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Create the repository (if it doesn't already exist)\n",
        "print(f\"Attempting to create/check repository: {REPO_ID}\")\n",
        "try:\n",
        "    # Use the logged-in credentials to create the repo\n",
        "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "    print(f\"Repository {REPO_ID} checked/created successfully.\")\n",
        "except Exception as e:\n",
        "    # This should ONLY fail now if the REPO_ID format is wrong\n",
        "    print(f\"Error creating repo: {e}\")\n",
        "\n",
        "# Upload all files\n",
        "print(f\"\\nStarting upload from {MERGED_MODEL_DIR} to {REPO_ID}...\")\n",
        "api.upload_folder(\n",
        "    folder_path=MERGED_MODEL_DIR,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Final merged Phi-2 model after SemEval fine-tuning.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- UPLOAD COMPLETE AND SUCCESSFUL! ---\")\n",
        "print(f\"Your fine-tuned model is now public here (use this link for your paper):\")\n",
        "print(f\"https://huggingface.co/{REPO_ID}\")"
      ],
      "metadata": {
        "id": "Pr1c_QFszoIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING OUTPUT FILE"
      ],
      "metadata": {
        "id": "k0JRjsGB9ZQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- 0. Installation ---\n",
        "print(\"Installing required packages...\")\n",
        "# Ensure transformers and accelerate are installed\n",
        "!pip install -q transformers accelerate torch pandas\n",
        "\n",
        "# --- 1. Configuration (MUST VERIFY) ---\n",
        "ID_COLUMN = \"id\"\n",
        "WORD1_COLUMN = \"word1\"\n",
        "WORD2_COLUMN = \"word2\"\n",
        "HEADLINE_COLUMN = \"headline\"\n",
        "\n",
        "# ðŸ’¥ VERIFY: Upload your test file to the /content/ directory first!\n",
        "INPUT_TSV_FILE = \"/content/task-a-en.tsv\"\n",
        "OUTPUT_JSONL_FILE = \"sem_eval_predictions.jsonl\"\n",
        "\n",
        "# ðŸ’¥ CRITICAL: Use your Phi-2 model ID (Fixing casing/typo from previous attempts)\n",
        "MERGED_REPO_ID = \"insaabbas/phi2_humor_merged_model\"\n",
        "\n",
        "# ðŸ’¥ CRITICAL: Use the correct, clean Phi-2 prompt template\n",
        "HUMOR_PROMPT_TEMPLATE = \"### Input: {input_text}\\n### Joke: \"\n",
        "\n",
        "# --- 2. Model Loading ---\n",
        "\n",
        "print(f\"\\nLoading final merged Phi-2 model from: {MERGED_REPO_ID}\")\n",
        "\n",
        "try:\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MERGED_REPO_ID,\n",
        "        torch_dtype=torch.bfloat16, # Use bfloat16 for Phi-2\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MERGED_REPO_ID, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Setup Inference Pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "    print(\"Model loaded successfully. Starting inference...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nFATAL ERROR: Could not load model. Check the model ID or try again later. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Joke Generation Function ---\n",
        "\n",
        "def generate_joke(input_text, generator, template):\n",
        "    \"\"\"Generates a joke using the specified template and pipeline.\"\"\"\n",
        "    # Construct the prompt, leaving the joke text field empty\n",
        "    prompt = template.format(input_text=input_text)\n",
        "\n",
        "    # Simple check for invalid input to prevent model crash\n",
        "    if input_text == \"ERROR: NO VALID INPUT FOUND\":\n",
        "        return \"Not available due to missing input data.\"\n",
        "\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=64, # 64 tokens is usually enough for a short joke\n",
        "        do_sample=True,\n",
        "        temperature=0.85,\n",
        "        top_p=0.9,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text'].strip()\n",
        "\n",
        "    # Clean up the output based on the template structure\n",
        "    joke = generated_text.split(\"### Joke:\", 1)[-1].strip() if \"### Joke:\" in generated_text else generated_text\n",
        "\n",
        "    # Final cleanup (get only the first line of the joke)\n",
        "    return joke.split('\\n')[0].strip()\n",
        "\n",
        "# --- 4. Main Processing Function ---\n",
        "\n",
        "def process_and_save_predictions(input_file, output_file, id_col, headline_col, word1_col, word2_col):\n",
        "    \"\"\"Reads TSV, dynamically generates inputs, generates jokes, and saves results to JSONL.\"\"\"\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"\\nFATAL ERROR: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Reading input data from: {input_file}\")\n",
        "\n",
        "    # Reading TSV, using keep_default_na=False to treat '-' and empty strings as data\n",
        "    df = pd.read_csv(input_file, sep='\\t', keep_default_na=False)\n",
        "\n",
        "    required_cols = [id_col, headline_col, word1_col, word2_col]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Error: Required columns {required_cols} not found in the file.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Generating jokes for {len(df)} inputs...\")\n",
        "    predictions = []\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Jokes\"):\n",
        "        input_id = row[id_col]\n",
        "        input_text = \"\"\n",
        "\n",
        "        # --- Dynamic Input Selection Logic ---\n",
        "        # Strip any leading/trailing whitespace\n",
        "        headline = str(row.get(headline_col, '')).strip()\n",
        "        word1 = str(row.get(word1_col, '')).strip()\n",
        "        word2 = str(row.get(word2_col, '')).strip()\n",
        "\n",
        "        # Prioritize the headline, as it provides more context\n",
        "        if headline and headline != '-':\n",
        "            input_text = headline\n",
        "        elif word1 and word2 and word1 != '-' and word2 != '-':\n",
        "            # Combine words separated by a space\n",
        "            input_text = f\"{word1} {word2}\"\n",
        "        else:\n",
        "            # Fallback for completely empty or malformed rows\n",
        "            input_text = \"ERROR: NO VALID INPUT FOUND\"\n",
        "\n",
        "\n",
        "        # Generate the joke\n",
        "        generated_joke = generate_joke(input_text, generator, HUMOR_PROMPT_TEMPLATE)\n",
        "\n",
        "        # Store the result in the submission format\n",
        "        predictions.append({\n",
        "            \"id\": input_id,\n",
        "            \"prediction\": generated_joke\n",
        "        })\n",
        "\n",
        "    # Save the results to JSONL\n",
        "    print(f\"Saving predictions to: {output_file}\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        for item in predictions:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    print(f\"\\n--- SUCCESS ---\")\n",
        "    print(f\"The final prediction file is saved as: {OUTPUT_JSONL_FILE}\")\n",
        "    print(\"ACTION REQUIRED: Zip this file and upload the ZIP archive to the SemEval CodaLab platform.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "process_and_save_predictions(INPUT_TSV_FILE, OUTPUT_JSONL_FILE, ID_COLUMN, HEADLINE_COLUMN, WORD1_COLUMN, WORD2_COLUMN)"
      ],
      "metadata": {
        "id": "loWW7L6m7huQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this command in a Colab cell after the Python script finishes\n",
        "!zip sem_eval_predictions.zip sem_eval_predictions.jsonl"
      ],
      "metadata": {
        "id": "Uwulaluh8HCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}